{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Motion_Keypoint.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1nEZ5nRNMVFJU4LdOM-QgTuSkY7j0s2_h","authorship_tag":"ABX9TyM7ooQMwDnN0ATTxnaamU+E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PVPORpYgraGD"},"source":["# Motion_Keypoint_Baseline"]},{"cell_type":"markdown","metadata":{"id":"qQ5O-d83Y7l2"},"source":["### Module Mount & Data Load"]},{"cell_type":"code","metadata":{"id":"xiVChnFFaw-W"},"source":["import cv2\r\n","import glob\r\n","import matplotlib.pyplot as plt\r\n","import numpy as np\r\n","import os\r\n","import pandas as pd\r\n","import random\r\n","import shutil\r\n","import tensorflow as tf\r\n","\r\n","from keras.layers.advanced_activations import LeakyReLU, PReLU\r\n","from math import cos, sin, pi\r\n","from PIL import Image\r\n","from tqdm import tqdm\r\n","from tensorflow.keras import Sequential, Model\r\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\r\n","from tensorflow.keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D, ZeroPadding2D, GlobalAveragePooling2D\r\n","from tensorflow.keras.models import load_model\r\n","from tensorflow.keras.optimizers import Adam"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JN5cNKPBbILg"},"source":["# 경로 이동\r\n","os.chdir('/content/drive/MyDrive/Colab/Dacon/Motion_Keypoint/data') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PFtkUP1wckBD"},"source":["# train.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pp7WxRtIcqPf"},"source":["# 해당 코드는 아래 Train, Valid Split 이후에 실행\r\n","train = pd.read_csv('./train.csv')\r\n","valid = pd.read_csv('./valid.csv')\r\n","display(train.head(2))\r\n","display(valid.head(2))\r\n","\r\n","train_paths = glob.glob('./train/*.jpg')\r\n","valid_paths = glob.glob('./valid/*.jpg')\r\n","test_paths = glob.glob('./test_imgs/*.jpg')\r\n","print(len(train_paths), len(valid_paths), len(test_paths))\r\n","\r\n","train_paths.sort()\r\n","valid_paths.sort()\r\n","test_pats.sort()\r\n","\r\n","train['path'] = train_paths\r\n","valid['path'] = valid_paths"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XNMLhaU0aGgv"},"source":["### Train, Valid Split"]},{"cell_type":"code","metadata":{"id":"ZQ4y1N-QW15L"},"source":["# train, val folder 생성\r\n","root_dir = '/content/drive/My Drive/Colab/Dacon/Motion_Keypoint/data'\r\n","\r\n","os.makedirs(root_dir +'/train')\r\n","os.makedirs(root_dir +'/val')\r\n","\r\n","# validation용 파일은 10% 비율로 random sampling\r\n","# random.seed() 넣으시면 복원 가능\r\n","src = \"train_imgs\"\r\n","all_filename = os.listdir(src)\r\n","valid_filename = random.sample(all_filename, int(len(train_all) * 0.1))\r\n","train_filename = [x for x in all_filename if x not in valid_filename]\r\n","\r\n","print(len(train_filename), len(valid_filename))\r\n","\r\n","train_filename = [src+'/'+ name for name in train_filename]\r\n","valid_filename = [src+'/' + name for name in valid_filename]\r\n","\r\n","print('Total images: ', len(all_filename))\r\n","print('Training: ', len(train_filename))\r\n","print('Validation: ', len(valid_filename))\r\n","\r\n","# copy & paste images\r\n","for name in tqdm(train_filename):\r\n","    shutil.copy(name, 'train')\r\n","\r\n","for name in tqdm(valid_filename):\r\n","    shutil.copy(name, 'val')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0NjamAo6VnfC"},"source":["# train, valid folder 속 모든 이미지 파일 read & sort\r\n","train_paths = glob.glob('./train/*.jpg')\r\n","valid_paths = glob.glob('./valid/*.jpg')\r\n","train_paths.sort()\r\n","valid_paths.sort()\r\n","\r\n","train_filename = []\r\n","valid_filename = []\r\n","\r\n","for t_paths in tqdm(train_paths):\r\n","    filename = t_paths.split('/')[-1]\r\n","    train_filename.append(filename)\r\n","\r\n","for v_paths in tqdm(valid_paths):\r\n","    filename = v_paths.split('/')[-1]\r\n","    valid_filename.append(filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjHZ4r8RWGgm"},"source":["# 각각의 train, valid 이미지들의 정보만을 담고 있는 DataFrame 생성\r\n","train_df = train[train['image'].isin(train_filename)]\r\n","train_df.reset_index(inplace=True, drop=True)\r\n","\r\n","valid_df = train[train['image'].isin(valid_filename)]\r\n","valid_df.reset_index(inplace=True, drop=True)\r\n","\r\n","train_df.to_csv('train.csv', index=False)\r\n","valid_df.to_csv('valid.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6N5SgmWaaNbI"},"source":["### 시각화"]},{"cell_type":"code","metadata":{"id":"_MWkTSqMdA9J"},"source":["plt.figure(figsize=(40,20))\r\n","count=1\r\n","\r\n","for i in np.random.randint(0,len(train_paths),5):\r\n","    \r\n","    plt.subplot(5,1, count)\r\n","    \r\n","    img_sample_path = train_paths[i]\r\n","    img = Image.open(img_sample_path)\r\n","    img_np = np.array(img)\r\n","\r\n","    keypoint = train.iloc[:,1:49] #위치 키포인트 하나씩 확인\r\n","    keypoint_sample = keypoint.iloc[i, :]\r\n","    \r\n","    for j in range(0,len(keypoint.columns),2):\r\n","        plt.plot(keypoint_sample[j], keypoint_sample[j+1],'rx')\r\n","        plt.imshow(img_np)\r\n","    \r\n","    count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W01kd-M9Uazm"},"source":["### Augmentation"]},{"cell_type":"code","metadata":{"id":"UGTnWdCIwEJR"},"source":["# Augmentation Setting\r\n","pixel_shifts = [12]\r\n","rotation_angles = [12]\r\n","inc_brightness_ratio = 1.2\r\n","dec_brightness_ratio = 0.8\r\n","noise_ratio = 0.008"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hbPNQUydN_cd"},"source":["# 좌우 반전\r\n","def left_right_flip(images, keypoints):\r\n","    flipped_keypoints = []\r\n","    flipped_images = np.flip(images, axis=1)\r\n","    for idx, sample_keypoints in enumerate(keypoints):\r\n","        if idx%2 == 0:\r\n","            flipped_keypoints.append(480.-sample_keypoints)\r\n","        else:\r\n","            flipped_keypoints.append(sample_keypoints)\r\n","    return flipped_images, flipped_keypoints"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7I6_dOKuWc3"},"source":["# 수직/수평 동시 이동\r\n","# forloop에서 shift_x, shift_y 중 하나만 놓으면\r\n","# 수직 또는 수평 이동만 따로 시행 가능\r\n","def shift_images(images, keypoints):\r\n","    # tensor -> numpy\r\n","    images = images.numpy()\r\n","    shifted_images = []\r\n","    shifted_keypoints = []\r\n","    for shift in pixel_shifts:   \r\n","        for (shift_x,shift_y) in [(-shift,-shift),(-shift,shift),(shift,-shift),(shift,shift)]:\r\n","            # 이동할 matrix 생성\r\n","            M = np.float32([[1,0,shift_x],[0,1,shift_y]])\r\n","            shifted_keypoint = np.array([])\r\n","            shifted_x_list = np.array([])\r\n","            shifted_y_list = np.array([])\r\n","            # 이미지 이동\r\n","            shifted_image = cv2.warpAffine(images, M, (480,270), flags=cv2.INTER_CUBIC)\r\n","            # 이동한만큼 keypoint 수정\r\n","            for idx, point in enumerate(keypoints):\r\n","                if idx%2 == 0: \r\n","                    shifted_keypoint = np.append(shifted_keypoint, point+shift_x)\r\n","                    shifted_x_list = np.append(shifted_x_list, point+shift_x)\r\n","                else: \r\n","                    shifted_keypoint =np.append(shifted_keypoint, point+shift_y)\r\n","                    shifted_y_list = np.append(shifted_y_list, point+shift_y)\r\n","            # 수정된 keypoint가 이미지 사이즈를 벗어나지 않으면 append\r\n","            if np.all(0.0<shifted_x_list) and np.all(shifted_x_list<480) and np.all(0.0<shifted_y_list) and np.all(shifted_y_list<270):\r\n","                shifted_images.append(shifted_image.reshape(270,480,3))\r\n","                shifted_keypoints.append(shifted_keypoint)\r\n","\r\n","    return shifted_images, shifted_keypoints"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wq5N7QzISEd"},"source":["# 이미지 회전\r\n","def rotate_augmentation(images, keypoints):\r\n","    # tensor -> numpy\r\n","    images = images.numpy()\r\n","    rotated_images = []\r\n","    rotated_keypoints = []\r\n","    \r\n","    for angle in rotation_angles:\r\n","        for angle in [angle,-angle]:\r\n","            # 회전할 matrix 생성\r\n","            M = cv2.getRotationMatrix2D((240,135), angle, 1.0)\r\n","            # cv2_imshow로는 문제없지만 추후 plt.imshow로 사진을 확인할 경우 black screen 생성...\r\n","            # 혹시 몰라 matrix를 ndarray로 변환\r\n","            M = np.array(M, dtype=np.float32)\r\n","            angle_rad = -angle*pi/180\r\n","            rotated_image = cv2.warpAffine(images, M, (480,270))\r\n","            rotated_images.append(rotated_image)\r\n","            \r\n","            # keypoint를 copy하여 forloop상에서 값이 계속 없데이트 되는 것을 회피\r\n","            rotated_keypoint = keypoints.copy()\r\n","            rotated_keypoint[0::2] = rotated_keypoint[0::2] - 240\r\n","            rotated_keypoint[1::2] = rotated_keypoint[1::2] - 135\r\n","            \r\n","            for idx in range(0,len(rotated_keypoint),2):\r\n","                rotated_keypoint[idx] = rotated_keypoint[idx]*cos(angle_rad)-rotated_keypoint[idx+1]*sin(angle_rad)\r\n","                rotated_keypoint[idx+1] = rotated_keypoint[idx]*sin(angle_rad)+rotated_keypoint[idx+1]*cos(angle_rad)\r\n","\r\n","            rotated_keypoint[0::2] = rotated_keypoint[0::2] + 240\r\n","            rotated_keypoint[1::2] = rotated_keypoint[1::2] + 135\r\n","            rotated_keypoints.append(rotated_keypoint)\r\n","        \r\n","    return rotated_images, rotated_keypoints"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zkLn-CnyreGF"},"source":["# 이미지 해상도 조절\r\n","def alter_brightness(images):\r\n","    altered_brightness_images = []\r\n","    inc_brightness_images = np.clip(images*inc_brightness_ratio, 0.0, 1.0)\r\n","    dec_brightness_images = np.clip(images*dec_brightness_ratio, 0.0, 1.0)\r\n","    altered_brightness_images.append(inc_brightness_images)\r\n","    altered_brightness_images.append(dec_brightness_images)\r\n","    return altered_brightness_images"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6F2OJFumvKR8"},"source":["def add_noise(images):\r\n","    images = images.numpy()\r\n","    noise = noise_ratio * np.random.randn(270,480,3)\r\n","    noise = noise.astype(np.float32)\r\n","    # 생성한 noise를 원본에 add\r\n","    noisy_image = cv2.add(images, noise)\r\n","    return noisy_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kVQ9SRpvfkhS"},"source":["### Generator"]},{"cell_type":"code","metadata":{"id":"dvLxZDpzIeuK"},"source":["def trainGenerator():\r\n","    # 원본 이미지 resize\r\n","    for i in range(len(train)):\r\n","        img = tf.io.read_file(train['path'][i]) # path(경로)를 통해 이미지 읽기\r\n","        img = tf.image.decode_jpeg(img, channels=3) # 경로를 통해 불러온 이미지를 tensor로 변환\r\n","        img = tf.image.resize(img, [270,480]) # 이미지 resize \r\n","        img = img/255                         # 이미지 rescaling\r\n","        target = train.iloc[:,1:49].iloc[i,:] # keypoint 뽑아주기\r\n","        target = target/4\r\n","\r\n","        yield (img, target)\r\n","    \r\n","    # horizontal flip\r\n","    for i in range(len(train)):\r\n","        img = tf.io.read_file(train['path'][i]) \r\n","        img = tf.image.decode_jpeg(img, channels=3) \r\n","        img = tf.image.resize(img, [270,480]) \r\n","        img = img/255\r\n","        target = train.iloc[:,1:49].iloc[i,:] \r\n","        target = target/4\r\n","        img, target = left_right_flip(img, target)\r\n","        \r\n","        yield (img, target)\r\n","\r\n","    # Horizontal & Vertical shift\r\n","    for i in range(len(train)):\r\n","        img = tf.io.read_file(train['path'][i])\r\n","        img = tf.image.decode_jpeg(img, channels=3)\r\n","        img = tf.image.resize(img, [270,480])\r\n","        img = img/255\r\n","        target = train.iloc[:,1:49].iloc[i,:]\r\n","        target = target/4\r\n","        img_list, target_list = shift_images(img, target)\r\n","        for shifted_img, shifted_target in zip(img_list, target_list):\r\n","            \r\n","            yield (shifted_img, shifted_target)\r\n","\r\n","    # Rotation\r\n","    for i in range(len(train)):\r\n","        img = tf.io.read_file(train['path'][i])\r\n","        img = tf.image.decode_jpeg(img, channels=3)\r\n","        img = tf.image.resize(img, [270,480])\r\n","        img = img/255\r\n","        target = train.iloc[:,1:49].iloc[i,:]\r\n","        target = target/4\r\n","        img_list, target_list = rotate_augmentation(img, target)\r\n","        for rotated_img, rotated_target in zip(img_list, target_list):\r\n","            \r\n","            yield (rotated_img, rotated_target)\r\n","\r\n","    # Alter_Brightness\r\n","    for i in range(len(train)):\r\n","        img = tf.io.read_file(train['path'][i])\r\n","        img = tf.image.decode_jpeg(img, channels=3)\r\n","        img = tf.image.resize(img, [270,480])\r\n","        img = img/255\r\n","        target = train.iloc[:,1:49].iloc[i,:]\r\n","        target = target/4\r\n","        img_list = alter_brightness(img)\r\n","        for altered_brightness_images in img_list:\r\n","            \r\n","            yield (altered_brightness_images, target)\r\n","\r\n","    # Adding_Noise\r\n","    for i in range(len(train)):\r\n","        img = tf.io.read_file(train['path'][i])\r\n","        img = tf.image.decode_jpeg(img, channels=3)\r\n","        img = tf.image.resize(img, [270,480])\r\n","        img = img/255\r\n","        target = train.iloc[:,1:49].iloc[i,:]\r\n","        target = target/4\r\n","        noisy_img = add_noise(img)\r\n","\r\n","        yield (noisy_img, target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ocMXeBkxWPjE"},"source":["def validGenerator():\r\n","    # 원본 이미지 resize\r\n","    for i in range(len(valid)):\r\n","        img = tf.io.read_file(valid['path'][i]) # path(경로)를 통해 이미지 읽기\r\n","        img = tf.image.decode_jpeg(img, channels=3) # 경로를 통해 불러온 이미지를 tensor로 변환\r\n","        img = tf.image.resize(img, [270,480]) # 이미지 resize \r\n","        img = img/255                         # 이미지 rescaling\r\n","        target = valid.iloc[:,1:49].iloc[i,:] # keypoint 뽑아주기\r\n","        target = target/4\r\n","\r\n","        yield (img, target)\r\n","    \r\n","    # Horizontal flip\r\n","    for i in range(len(valid)):\r\n","        img = tf.io.read_file(valid['path'][i]) \r\n","        img = tf.image.decode_jpeg(img, channels=3) \r\n","        img = tf.image.resize(img, [270,480]) \r\n","        img = img/255\r\n","        target = valid.iloc[:,1:49].iloc[i,:] \r\n","        target = target/4\r\n","        img, target = left_right_flip(img, target)\r\n","        \r\n","        yield (img, target)\r\n","\r\n","    # Horizontal & Vertical shift\r\n","    for i in range(len(valid)):\r\n","        img = tf.io.read_file(valid['path'][i])\r\n","        img = tf.image.decode_jpeg(img, channels=3)\r\n","        img = tf.image.resize(img, [270,480])\r\n","        img = img/255\r\n","        target = valid.iloc[:,1:49].iloc[i,:]\r\n","        target = target/4\r\n","        img_list, target_list = shift_images(img, target)\r\n","        for shifted_img, shifted_target in zip(img_list, target_list):\r\n","            \r\n","            yield (shifted_img, shifted_target)\r\n","\r\n","    # Rotation\r\n","    for i in range(len(valid)):\r\n","        img = tf.io.read_file(valid['path'][i])\r\n","        img = tf.image.decode_jpeg(img, channels=3)\r\n","        img = tf.image.resize(img, [270,480])\r\n","        img = img/255\r\n","        target = valid.iloc[:,1:49].iloc[i,:]\r\n","        target = target/4\r\n","        img_list, target_list = rotate_augmentation(img, target)\r\n","        for rotated_img, rotated_target in zip(img_list, target_list):\r\n","            \r\n","            yield (rotated_img, rotated_target)\r\n","\r\n","    # Alter_Brightness\r\n","    for i in range(len(valid)):\r\n","        img = tf.io.read_file(valid['path'][i])\r\n","        img = tf.image.decode_jpeg(img, channels=3)\r\n","        img = tf.image.resize(img, [270,480])\r\n","        img = img/255\r\n","        target = valid.iloc[:,1:49].iloc[i,:]\r\n","        target = target/4\r\n","        img_list = alter_brightness(img)\r\n","        for altered_brightness_images in img_list:\r\n","            \r\n","            yield (altered_brightness_images, target)\r\n","\r\n","    # Adding_Noise\r\n","    for i in range(len(valid)):\r\n","        img = tf.io.read_file(valid['path'][i])\r\n","        img = tf.image.decode_jpeg(img, channels=3)\r\n","        img = tf.image.resize(img, [270,480])\r\n","        img = img/255\r\n","        target = valid.iloc[:,1:49].iloc[i,:]\r\n","        target = target/4\r\n","        noisy_img = add_noise(img)\r\n","\r\n","        yield (noisy_img, target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eYUf2ulVJGE0"},"source":["batch_size = 64\r\n","\r\n","train_dataset = tf.data.Dataset.from_generator(trainGenerator, (tf.float32, tf.float32), (tf.TensorShape([270,480,3]),tf.TensorShape([48])))\r\n","train_dataset = train_dataset.batch(batch_size).prefetch(1)\r\n","valid_dataset = tf.data.Dataset.from_generator(validGenerator, (tf.float32, tf.float32), (tf.TensorShape([270,480,3]),tf.TensorShape([48])))\r\n","valid_dataset = valid_dataset.batch(batch_size).prefetch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KpEmq5l5a9Gm"},"source":["### Baseline Modeling"]},{"cell_type":"code","metadata":{"id":"5tCmVTRxhD5I"},"source":["# Callback 설정\r\n","earlystop = EarlyStopping(patience=7)\r\n","learning_rate_reduction=ReduceLROnPlateau(\r\n","                        monitor= \"val_loss\", \r\n","                        patience = 2, \r\n","                        factor = 0.85, \r\n","                        min_lr=1e-7,\r\n","                        verbose=1)\r\n","\r\n","model_check = ModelCheckpoint( #에포크마다 현재 가중치를 저장    \r\n","        filepath=\"./baseline_with_augmentation_he.h5\", #모델 파일 경로\r\n","        monitor='val_loss',  # val_loss 가 좋아지지 않으면 모델 파일을 덮어쓰지 않음.\r\n","        save_best_only=True)\r\n","\r\n","callbacks = [earlystop, learning_rate_reduction, model_check]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UWt17FcZVW1x"},"source":["# Model Structure\r\n","model = Sequential()\r\n","\r\n","model.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=(270,480,3)))\r\n","model.add(LeakyReLU(alpha = 0.1))\r\n","model.add(BatchNormalization())\r\n","model.add(Convolution2D(32, (3,3), padding='same', use_bias=False))\r\n","model.add(LeakyReLU(alpha = 0.1))\r\n","model.add(BatchNormalization())\r\n","model.add(MaxPool2D(pool_size=(2, 2)))\r\n","\r\n","\r\n","model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\r\n","model.add(LeakyReLU(alpha = 0.1))\r\n","model.add(BatchNormalization())\r\n","model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\r\n","model.add(LeakyReLU(alpha = 0.1))\r\n","model.add(BatchNormalization())\r\n","model.add(MaxPool2D(pool_size=(2, 2)))\r\n","\r\n","\r\n","model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\r\n","model.add(LeakyReLU(alpha = 0.1))\r\n","model.add(BatchNormalization())\r\n","model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\r\n","model.add(LeakyReLU(alpha = 0.1))\r\n","model.add(BatchNormalization())\r\n","model.add(MaxPool2D(pool_size=(2, 2)))\r\n","\r\n","\r\n","model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\r\n","model.add(LeakyReLU(alpha = 0.1))\r\n","model.add(BatchNormalization())\r\n","model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\r\n","model.add(LeakyReLU(alpha = 0.1))\r\n","model.add(BatchNormalization())\r\n","model.add(MaxPool2D(pool_size=(2, 2)))\r\n","\r\n","\r\n","model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\r\n","model.add(LeakyReLU(alpha = 0.1))\r\n","model.add(BatchNormalization())\r\n","model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\r\n","model.add(LeakyReLU(alpha = 0.1))\r\n","model.add(BatchNormalization())\r\n","model.add(MaxPool2D(pool_size=(2, 2)))\r\n","\r\n","\r\n","model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\r\n","model.add(LeakyReLU(alpha = 0.1))\r\n","model.add(BatchNormalization())\r\n","model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\r\n","model.add(LeakyReLU(alpha = 0.1))\r\n","model.add(BatchNormalization())\r\n","\r\n","\r\n","model.add(Flatten())\r\n","model.add(Dense(512,activation='relu'))\r\n","model.add(Dropout(0.5))\r\n","model.add(Dense(48))\r\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NU2LA5pzVgxT"},"source":["model.compile(optimizer=Adam(learning_rate=0.0001), \r\n","              loss='mean_squared_error',\r\n","              metrics=['mae'])\r\n","\r\n","history = model.fit(train_dataset,\r\n","                    epochs=1000,\r\n","                    validation_data=valid_dataset,\r\n","                    callbacks = callbacks,\r\n","                    verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IakZTakJep7f"},"source":["### Pre-trained"]},{"cell_type":"code","metadata":{"id":"0-6g6k1zY4fZ"},"source":["from tensorflow.keras import models\r\n","from tensorflow.keras.applications import ResNet152V2 \r\n","\r\n","resnet152 = ResNet152V2(weights ='imagenet', include_top = False, \r\n","                       input_shape = (270,480,3))\r\n","\r\n","earlystop = EarlyStopping(patience=7)\r\n","learning_rate_reduction=ReduceLROnPlateau(\r\n","                        monitor= \"val_loss\", \r\n","                        patience = 3, \r\n","                        factor = 0.85, \r\n","                        min_lr=1e-7,\r\n","                        verbose=1)\r\n","\r\n","model_check = ModelCheckpoint( #에포크마다 현재 가중치를 저장    \r\n","        filepath=\"./resnet152.h5\", #모델 파일 경로\r\n","        monitor='val_loss',  # val_loss 가 좋아지지 않으면 모델 파일을 덮어쓰지 않음.\r\n","        save_best_only=True)\r\n","\r\n","callbacks = [earlystop, learning_rate_reduction, model_check]\r\n","\r\n","# \r\n","for layer in resnet152.layers:\r\n","    layer.trainable = True\r\n","\r\n","model = models.Sequential()\r\n","model.add(resnet152)\r\n","model.add(Flatten())\r\n","model.add(Dense(512, activation='relu'))\r\n","model.add(BatchNormalization())\r\n","model.add(Dropout(0.5))\r\n","model.add(Dense(48)) \r\n","model.summary()\r\n","\r\n","model.compile(loss='mean_squared_error',\r\n","                optimizer=Adam(learning_rate=0.0001),\r\n","                metrics=['mae'])\r\n","\r\n","history = model.fit(train_dataset,\r\n","                    epochs=150,\r\n","                    validation_data=valid_dataset,\r\n","                    callbacks = callbacks)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xzGpu6AXcB-R"},"source":["### Load Model & Fine Tuning"]},{"cell_type":"code","metadata":{"id":"JVJs-rdQN8zc"},"source":["saved_model = load_model('/content/drive/MyDrive/Colab/Dacon/Motion_Keypoint/data/baseline_with_augmentation_noise.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F8BY1yI8OVZv"},"source":["earlystop = EarlyStopping(patience=7)\r\n","learning_rate_reduction=ReduceLROnPlateau(\r\n","                        monitor= \"val_loss\", \r\n","                        patience = 3, \r\n","                        factor = 0.95, \r\n","                        min_lr=1e-7,\r\n","                        verbose=1)\r\n","\r\n","model_check = ModelCheckpoint( #에포크마다 현재 가중치를 저장    \r\n","        filepath=\"./baseline_noise.h5\", #모델 파일 경로\r\n","        monitor='val_loss',  # val_loss 가 좋아지지 않으면 모델 파일을 덮어쓰지 않음.\r\n","        save_best_only=True)\r\n","\r\n","callbacks = [earlystop, learning_rate_reduction, model_check]\r\n","\r\n","for layer in saved_model.layers:\r\n","    layer.trainable = True\r\n","\r\n","saved_model.summary()\r\n","saved_model.compile(loss='mean_squared_error',\r\n","                optimizer=Adam(learning_rate=0.00004),\r\n","                metrics=['mae'])\r\n","\r\n","history = saved_model.fit(train_dataset,\r\n","                          epochs=150,\r\n","                          validation_data=valid_dataset,\r\n","                          callbacks = callbacks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5EObxLtG1BoO"},"source":["saved_model = load_model('/content/drive/MyDrive/Colab/Dacon/Motion_Keypoint/data/baseline_with_augmentation_rotation.h5')\r\n","\r\n","model = Sequential()\r\n","for layer in saved_model.layers[:-4]: # go through until last layer\r\n","    model.add(layer)\r\n","model.add(GlobalAveragePooling2D())\r\n","model.add(Dense(512,activation='relu'))\r\n","model.add(Dropout(0.5))\r\n","model.add(Dense(48))\r\n","model.summary()\r\n","\r\n","earlystop = EarlyStopping(patience=5)\r\n","learning_rate_reduction=ReduceLROnPlateau(\r\n","                        monitor= \"val_loss\", \r\n","                        patience = 2, \r\n","                        factor = 0.5, \r\n","                        min_lr=1e-7,\r\n","                        verbose=1)\r\n","\r\n","model_check = ModelCheckpoint( #에포크마다 현재 가중치를 저장    \r\n","        filepath=\"./baseline_with_augmentation_GAP.h5\", #모델 파일 경로\r\n","        monitor='val_loss',  # val_loss 가 좋아지지 않으면 모델 파일을 덮어쓰지 않음.\r\n","        save_best_only=True)\r\n","\r\n","callbacks = [earlystop, learning_rate_reduction, model_check]\r\n","\r\n","model.compile(optimizer=Adam(learning_rate=0.0001), \r\n","              loss='mean_squared_error',\r\n","              metrics=['mae'])\r\n","\r\n","history = model.fit(train_dataset,\r\n","                    epochs=150,\r\n","                    validation_data=valid_dataset,\r\n","                    callbacks = callbacks,\r\n","                    verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VOH6s_YCLShH"},"source":["saved_model = load_model('/content/drive/MyDrive/Colab/Dacon/Motion_Keypoint/data/baseline_with_augmentation_noise.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M_K2oBAIcQe-"},"source":["### Load Test Set & Predict"]},{"cell_type":"code","metadata":{"id":"aE4fg_dBaNAR"},"source":["test_paths = glob.glob('./test_imgs/*.jpg')\r\n","test_paths.sort()\r\n","X_test=[]\r\n","\r\n","for test_path in tqdm(test_paths):\r\n","    img=tf.io.read_file(test_path)\r\n","    img=tf.image.decode_jpeg(img, channels=3)\r\n","    img=tf.image.resize(img, [270,480])\r\n","    img=img/255\r\n","    X_test.append(img)\r\n","\r\n","X_test=tf.stack(X_test, axis=0)\r\n","X_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z6GVLAqQaR1V"},"source":["# pred=model.predict(X_test)\r\n","pred=saved_model.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ILLFO8VvcHBV"},"source":["### Submission"]},{"cell_type":"code","metadata":{"id":"Isxqz6XcaTRd"},"source":["submission = pd.read_csv('./sample_submission.csv')\r\n","submission.iloc[:,1:]=pred * 4\r\n","submission"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H48nCNeap3w6"},"source":["bb = submission.iloc[1,1:49]\r\n","bb = np.array(bb)\r\n","aa = Image.open(test_paths[1])\r\n","plt.imshow(aa)\r\n","plt.scatter(bb[0::2], bb[1::2], marker='x')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XnqDaAKrabS8"},"source":["submission.to_csv('baseline_submission_noise.csv', index=False)"],"execution_count":null,"outputs":[]}]}